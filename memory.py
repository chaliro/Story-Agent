import os

from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

# --- ç¯å¢ƒè®¾ç½® (è¯·ç¡®ä¿æ‚¨çš„.envæ–‡ä»¶ä¸­æœ‰OPENAI_API_KEY) ---
from utils import get_embedding_llm

class MemorySystem:
    """
    ä¸€ä¸ªç®¡ç†å°è¯´åˆ›ä½œè®°å¿†çš„ç³»ç»Ÿï¼Œå®ç°äº†æŠ€æœ¯æŠ¥å‘Šä¸­çš„â€œæ‘˜è¦å±‚â€å’Œâ€œåˆ†å—å±‚â€ã€‚
    - æ‘˜è¦å±‚ (Summary Layer): å­˜å‚¨æ¯ä¸ªç« èŠ‚çš„æ‘˜è¦ï¼Œç”¨äºå¿«é€Ÿæ£€ç´¢é«˜çº§æƒ…èŠ‚ã€‚
    - åˆ†å—å±‚ (Chunk Layer): å­˜å‚¨æ¯ä¸ªç« èŠ‚çš„è¯¦ç»†æ–‡æœ¬å—ï¼Œç”¨äºæ£€ç´¢å…·ä½“æå†™å’Œå¯¹è¯ã€‚
    """

    def __init__(self, embedding_model=None):
        """
        åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿã€‚
        """
        print("Initializing MemorySystem...")
        self.embeddings = embedding_model if embedding_model else get_embedding_llm()

        # --- ã€æ ¸å¿ƒä¿®æ”¹ 2ã€‘å¢åŠ  chunk_size æ¥ä¼˜åŒ–è®°å¿†çš„å®Œæ•´æ€§ ---
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,      # å°†æ¯ä¸ªæ–‡æœ¬å—çš„å¤§å°ä»300å¢åŠ åˆ°800
            chunk_overlap=150,     # ç›¸åº”åœ°å¢åŠ é‡å éƒ¨åˆ†
            length_function=len,
            is_separator_regex=False,
        )

        # åˆå§‹åŒ–ä¸¤ä¸ªç©ºçš„å‘é‡æ•°æ®åº“
        embedding_dimension = len(self.embeddings.embed_query("test"))
        self.summary_store = FAISS.from_texts([" "], self.embeddings, metadatas=[{}])
        self.summary_store.delete(self.summary_store.index_to_docstore_id.values())
        self.full_text_store = FAISS.from_texts([" "], self.embeddings, metadatas=[{}])
        self.full_text_store.delete(self.full_text_store.index_to_docstore_id.values())

        print("MemorySystem initialized successfully.")

    def add_chapter(self, chapter_index: int, chapter_title: str, full_text: str, summary: str):
        """
        å°†ä¸€ä¸ªæ–°å®Œæˆçš„ç« èŠ‚æ·»åŠ åˆ°è®°å¿†ç³»ç»Ÿä¸­ã€‚
        è¿™åŒ…æ‹¬å°†å…¶æ‘˜è¦å­˜å…¥æ‘˜è¦åº“ï¼Œå¹¶å°†å…¨æ–‡åˆ†å—åå­˜å…¥åˆ†å—åº“ã€‚

        Args:
            chapter_index (int): ç« èŠ‚ç¼–å·ã€‚
            chapter_title (str): ç« èŠ‚æ ‡é¢˜ã€‚
            full_text (str): ç« èŠ‚çš„å®Œæ•´åŸæ–‡ã€‚
            summary (str): ç« èŠ‚çš„æ‘˜è¦ã€‚
        """
        print(f"--- Adding Chapter {chapter_index}: '{chapter_title}' to Memory ---")

        # [cite_start]1. æ·»åŠ åˆ°æ‘˜è¦å±‚ (Summary Layer) [cite: 123]
        summary_doc = Document(
            page_content=summary,
            metadata={
                "chapter_index": chapter_index,
                "chapter_title": chapter_title,
                "type": "chapter_summary"
            }
        )
        self.summary_store.add_documents([summary_doc])
        print(f"Added summary for Chapter {chapter_index} to the summary store.")

        # [cite_start]2. æ·»åŠ åˆ°åˆ†å—å±‚ (Chunk Layer) [cite: 137]
        # å¯¹å…¨æ–‡è¿›è¡Œåˆ†å—
        chunks = self.text_splitter.split_text(full_text)

        # [cite_start]ä¸ºæ¯ä¸ªåˆ†å—åˆ›å»º LangChain Document å¯¹è±¡ï¼Œå¹¶é™„å¸¦å…ƒæ•°æ® [cite: 145]
        chunk_docs = []
        for i, chunk in enumerate(chunks):
            doc = Document(
                page_content=chunk,
                metadata={
                    "chapter_index": chapter_index,
                    "chapter_title": chapter_title,
                    "chunk_index": i,
                    # chunk_type, emotional_tone ç­‰å…ƒæ•°æ®å¯é€šè¿‡LLMåˆ†ææ·»åŠ ï¼Œæ­¤å¤„ç®€åŒ–
                    "chunk_type": "å™è¿°"
                }
            )
            chunk_docs.append(doc)

        self.full_text_store.add_documents(chunk_docs)
        print(f"Split full text of Chapter {chapter_index} into {len(chunk_docs)} chunks and added to the full-text store.")
        print("-" * 20)

    def retrieve_context_for_writer(
        self,
        query: str,
        top_k_summaries: int = 10,
        top_k_chunks: int = 10
    ) -> str:
        """
        ä¸ºä½œå®¶Agentæ£€ç´¢ç›¸å…³çš„å†å²ä¸Šä¸‹æ–‡ã€‚
        å®ƒä¼šå¹¶è¡Œåœ°ä»æ‘˜è¦å±‚å’Œåˆ†å—å±‚æ£€ç´¢ä¿¡æ¯ï¼Œç„¶åæ•´åˆæˆä¸€ä»½ç»“æ„åŒ–çš„ä¸Šä¸‹æ–‡ã€‚
        [cite_start]è¿™æ¨¡æ‹Ÿäº†â€œèåˆæ£€ç´¢Agentâ€çš„ç®€åŒ–å·¥ä½œæµç¨‹ [cite: 156, 162]ã€‚

        Args:
            query (str): å½“å‰çš„åˆ›ä½œç›®æ ‡æˆ–ç« èŠ‚å¤§çº²ï¼Œç”¨äºæ£€ç´¢ã€‚
            top_k_summaries (int): è¦æ£€ç´¢çš„ç›¸å…³ç« èŠ‚æ‘˜è¦æ•°é‡ã€‚
            top_k_chunks (int): è¦æ£€ç´¢çš„ç›¸å…³æ–‡æœ¬åˆ†å—æ•°é‡ã€‚

        Returns:
            str: æ ¼å¼åŒ–åçš„å†å²ä¸Šä¸‹æ–‡ï¼Œå¯ç›´æ¥æ³¨å…¥åˆ°ä½œå®¶çš„Promptä¸­ã€‚
        """
        print(f"--- Retrieving Context for Query: '{query[:50]}...' ---")

        # å¹¶è¡Œæ£€ç´¢
        # [cite_start]ä»æ‘˜è¦å±‚æ£€ç´¢ï¼Œè·å–å®è§‚æƒ…èŠ‚çº¿ç´¢ [cite: 172]
        summary_results = self.summary_store.similarity_search_with_score(query, k=top_k_summaries)

        # [cite_start]ä»åˆ†å—å±‚æ£€ç´¢ï¼Œè·å–å…·ä½“çš„ç»†èŠ‚ã€å¯¹è¯å’Œæå†™ [cite: 173]
        chunk_results = self.full_text_store.similarity_search_with_score(query, k=top_k_chunks)

        # [cite_start]ç»“æœèåˆä¸æ ¼å¼åŒ–ï¼Œæ¨¡æ‹Ÿâ€œä¸Šä¸‹æ–‡å‹ç¼©ä¸æ³¨å…¥â€ [cite: 164, 181]
        context_parts = []

        if summary_results:
            context_parts.append("## ç›¸å…³å†å²ç« èŠ‚æ‘˜è¦å›é¡¾:")
            for doc, score in summary_results:
                metadata = doc.metadata
                context_parts.append(
                    f"- **ç¬¬{metadata['chapter_index']}ç« : {metadata['chapter_title']}** (ç›¸å…³åº¦: {score:.2f})\n"
                    f"  æ‘˜è¦: {doc.page_content}"
                )

        if chunk_results:
            context_parts.append("\n## è¿‡å»ç« èŠ‚ä¸­çš„ç›¸å…³ç»†èŠ‚ç‰‡æ®µ:")
            for doc, score in chunk_results:
                metadata = doc.metadata
                context_parts.append(
                    f"- **å‡ºè‡ªç¬¬{metadata['chapter_index']}ç« : {metadata['chapter_title']}** (ç›¸å…³åº¦: {score:.2f})\n"
                    f"  ç‰‡æ®µ: \"...{doc.page_content}...\""
                )

        if not context_parts:
            return "è®°å¿†åº“ä¸­æš‚æ— ç›¸å…³å†å²ä¿¡æ¯ã€‚"

        final_context = "\n".join(context_parts)
        print("Successfully retrieved and formatted context.")
        print("-" * 20)
        return final_context
    # +++ æ–°å¢æ–¹æ³• +++
    def save(self, directory: str):
        """å°†FAISSå‘é‡æ•°æ®åº“ä¿å­˜åˆ°æœ¬åœ°ç›®å½•ã€‚"""
        print(f"--- ğŸ’¾ Saving MemorySystem to {directory} ---")
        os.makedirs(directory, exist_ok=True)
        # ä½¿ç”¨FAISSçš„ä¸“ç”¨æ–¹æ³•ä¿å­˜ç´¢å¼•
        self.summary_store.save_local(os.path.join(directory, "summary_store"))
        self.full_text_store.save_local(os.path.join(directory, "full_text_store"))
        print("--- âœ… MemorySystem vector stores saved successfully. ---")

    # +++ æ–°å¢ç±»æ–¹æ³• +++
    @classmethod
    def load(cls, directory: str, embedding_model) -> "MemorySystem":
        """
        ä»æœ¬åœ°ç›®å½•åŠ è½½æˆ–åˆ›å»ºä¸€ä¸ªæ–°çš„MemorySystemå®ä¾‹ã€‚
        """
        summary_path = os.path.join(directory, "summary_store")
        full_text_path = os.path.join(directory, "full_text_store")

        # æ£€æŸ¥æŒä¹…åŒ–æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if os.path.exists(summary_path) and os.path.exists(full_text_path):
            print(f"--- ğŸ“‚ Loading MemorySystem from {directory} ---")
            # åˆ›å»ºä¸€ä¸ªæ–°çš„å®ä¾‹ï¼Œä½†ä¸è°ƒç”¨ __init__
            memory_system = super(MemorySystem, cls).__new__(cls)
            memory_system.embeddings = embedding_model
            memory_system.text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=150)

            # ä»æœ¬åœ°æ–‡ä»¶åŠ è½½FAISSç´¢å¼•
            memory_system.summary_store = FAISS.load_local(
                summary_path,
                embeddings=embedding_model,
                allow_dangerous_deserialization=True
            )
            memory_system.full_text_store = FAISS.load_local(
                full_text_path,
                embeddings=embedding_model,
                allow_dangerous_deserialization=True
            )
            print("--- âœ… MemorySystem loaded successfully from disk. ---")
            return memory_system
        else:
            print(f"--- âš ï¸ Memory store not found at '{directory}'. Creating a new one. ---")
            return cls(embedding_model=embedding_model)

if __name__ == "__main__":
    memory_system = MemorySystem()
    memory_system.save("./vector_memory/8")


