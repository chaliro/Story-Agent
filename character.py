import os
import json
import re
import dotenv
from typing import TypedDict, List, Dict, Optional, Any, Literal
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field
from langchain.output_parsers import PydanticOutputParser
from langchain.prompts import PromptTemplate
import pprint
import networkx as nx
from utils import get_llm, get_evaluation_llm, get_llm_user


# ================================================================= #
# 1. ç¯å¢ƒä¸æ¨¡å‹è®¾ç½® & å›¾è°±è·¯å¾„å·¥å…·
# ================================================================= #

def get_graph_path():
    """è·å–çŸ¥è¯†å›¾è°±(NetworkX)çš„æ–‡ä»¶è·¯å¾„"""
    dotenv.load_dotenv()
    return os.path.join(os.getenv("MEMORY_ROOT"), os.getenv("CURRENT_PROJECT_ID"), "story_graph.json")


def load_graph() -> nx.Graph:
    """åŠ è½½å›¾è°± (ä¿®å¤äº† FutureWarning)"""
    graph_path = get_graph_path()
    if os.path.exists(graph_path):
        try:
            with open(graph_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            # æ˜¾å¼æŒ‡å®š edges="links" ä»¥æ¶ˆé™¤è­¦å‘Š
            return nx.node_link_graph(data, edges="links")
        except Exception as e:
            print(f"âš ï¸ è¯»å–å›¾è°±å¤±è´¥ï¼Œå°†åˆ›å»ºæ–°å›¾: {e}")
            return nx.Graph()
    else:
        return nx.Graph()


def save_graph(G: nx.Graph):
    """ä¿å­˜å›¾è°± (ä¿®å¤äº† FutureWarning)"""
    graph_path = get_graph_path()
    # æ˜¾å¼æŒ‡å®š edges="links" ä»¥æ¶ˆé™¤è­¦å‘Š
    data = nx.node_link_data(G, edges="links")
    with open(graph_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


# ... (ç¬¬2éƒ¨åˆ†ï¼šæ•°æ®ç»“æ„å®šä¹‰ä¿æŒä¸å˜ï¼Œçœç•¥ä»¥èŠ‚çœç©ºé—´) ...
# ... (RelationshipStatus, CharacterProfile, KnowledgeBase, Pydanticæ¨¡å‹ç­‰) ...
RelationshipStatus = Literal[
    "æœ‹å‹", "æ‹äºº", "æ•Œäºº", "é™Œç”Ÿäºº", "å®¶åº­æˆå‘˜", "åˆä½œä¼™ä¼´", "ç«äº‰å¯¹æ‰‹", "åˆæ¬¡è§é¢", "åˆç§Ÿå®¤å‹"]


class CharacterProfile(TypedDict):
    """æ‰©å±•çš„è§’è‰²æ¡£æ¡ˆ (å·²æ›´æ–°å…³ç³»å­˜å‚¨ç»“æ„)"""
    name: str
    age: Optional[int]
    gender: Optional[str]
    backstory: str
    traits: List[str]  # æ€§æ ¼ç‰¹ç‚¹
    specialties: List[str]
    hobbies: List[str]
    occupations: List[str]
    appearance: str  # å¤–è²Œæè¿°
    # --- ç»“æ„æ›´æ–° ---
    # ä½¿ç”¨å­—å…¸å­˜å‚¨ï¼Œé”®ä¸ºå¯¹æ–¹è§’è‰²åï¼Œå€¼ä¸ºå…³ç³»è¯¦æƒ…
    relationship: Dict[str, Dict[str, str]]
    first_appearance_chapter: int  # é¦–æ¬¡å‡ºç°çš„ç« èŠ‚
    aliases: List[str]  # æ–°å¢åˆ«å


class KnowledgeBase(TypedDict):
    """çŸ¥è¯†åº“çš„é¡¶å±‚ç»“æ„"""
    characters: Dict[str, CharacterProfile]
    last_updated_chapter: int


# Pydantic æ¨¡å‹ç”¨äºè¾“å‡ºè§£æ
class CharacterInfo(BaseModel):
    """è§’è‰²ä¿¡æ¯æ¨¡å‹"""
    name: str = Field(description="è§’è‰²å§“å")
    estimated_age: str = Field(description="æ•°å­—/æœªçŸ¥")
    gender: str = Field(description="ç”·/å¥³/æœªçŸ¥")
    appearance: str = Field(description="å¤–è²Œæè¿°/æœªçŸ¥")
    traits: List[str] = Field(description="æ€§æ ¼ç‰¹ç‚¹åˆ—è¡¨")
    specialties: List[str] = Field(description="ç‰¹é•¿åˆ—è¡¨")
    hobbies: List[str] = Field(description="çˆ±å¥½åˆ—è¡¨")
    occupations: List[str] = Field(description="èŒä¸šåˆ—è¡¨")
    aliases: List[str] = Field(description="æœ¬ç« ä¸­å‡ºç°çš„è¯¥è§’è‰²çš„å…¶ä»–ç§°å‘¼/åˆ«å", default=[])


class CharacterInteraction(BaseModel):
    """è§’è‰²äº’åŠ¨æ¨¡å‹"""
    character_a: str = Field(description="è§’è‰²Aå§“å")
    character_b: str = Field(description="è§’è‰²Bå§“å")
    interaction_type: str = Field(description="å…³ç³»ç±»å‹")
    interaction_summary: str = Field(description="äº’åŠ¨æè¿°")


class CharacterAnalysisResult(BaseModel):
    """è§’è‰²åˆ†æç»“æœæ¨¡å‹"""
    characters: List[CharacterInfo] = Field(description="è§’è‰²åˆ—è¡¨")
    character_interactions: List[CharacterInteraction] = Field(description="è§’è‰²äº’åŠ¨åˆ—è¡¨")


class EvidenceText(BaseModel):
    """è¯æ®æ–‡æœ¬æ¨¡å‹"""
    evidence: str = Field(description="ä»ç« èŠ‚å†…å®¹ä¸­æå–çš„äº’åŠ¨æ–‡æœ¬ç‰‡æ®µ")


class BackstorySummary(BaseModel):
    """èƒŒæ™¯æ•…äº‹æ‘˜è¦æ¨¡å‹"""
    summary: str = Field(description="è§’è‰²ä¸ªäººèƒŒæ™¯æ•…äº‹çš„æ€»ç»“")


# ================================================================= #
# 3. åŸºç¡€å·¥å…·é›† (åˆå§‹åŒ–ã€è¯»å–ã€ä¿å­˜)
# ================================================================= #

def initialize_knowledge_base(KNOWLEDGE_BASE_PATH: str):
    if not os.path.exists(KNOWLEDGE_BASE_PATH):
        initial_kb = {"characters": {}, "last_updated_chapter": 0}
        with open(KNOWLEDGE_BASE_PATH, "w", encoding="utf-8") as f:
            json.dump(initial_kb, f, ensure_ascii=False, indent=2)
        print("âœ… å·²åˆå§‹åŒ–çŸ¥è¯†åº“æ–‡ä»¶")


def get_knowledge_base(KNOWLEDGE_BASE_PATH: str) -> KnowledgeBase:
    try:
        with open(KNOWLEDGE_BASE_PATH, "r", encoding="utf-8") as f:
            kb = json.load(f)
        if "characters" not in kb: kb["characters"] = {}
        if "last_updated_chapter" not in kb: kb["last_updated_chapter"] = 0
        for char_name in kb["characters"]:
            if "relationship" not in kb["characters"][char_name]:
                kb["characters"][char_name]["relationship"] = {}
            if "aliases" not in kb["characters"][char_name]:
                kb["characters"][char_name]["aliases"] = []
        return kb
    except FileNotFoundError:
        initialize_knowledge_base(KNOWLEDGE_BASE_PATH)
        return get_knowledge_base(KNOWLEDGE_BASE_PATH)


def save_knowledge_base(kb: KnowledgeBase, KNOWLEDGE_BASE_PATH: str):
    with open(KNOWLEDGE_BASE_PATH, "w", encoding="utf-8") as f:
        json.dump(kb, f, ensure_ascii=False, indent=2)


def clean_llm_response(content: str) -> str:
    if content.startswith("```json") and content.endswith("```"):
        content = content[len("```json"):-len("```")].strip()
    json_match = re.search(r"\{[\s\S]*\}", content)
    if json_match: content = json_match.group().strip()
    content = content.replace("'", '\"')
    return re.sub(r"//.*|/\*[\s\S]*?\*/", "", content)


# ================================================================= #
# 4. æ ¸å¿ƒé€»è¾‘å‡½æ•° (ä¿®å¤ç‰ˆ)
# ================================================================= #

def identify_characters_in_text(text: str, current_chapter: int, narrator_name: str = None) -> Dict[str, Any]:
    """ç¬¬ä¸€æ­¥ï¼šè¯†åˆ«æ–‡æœ¬ä¸­å‡ºç°çš„ä¸»è¦è§’è‰²"""

    # è·å–å·²çŸ¥è§’è‰²åˆ—è¡¨ (Entity Registry)
    dotenv.load_dotenv()
    knowledge_path = os.path.join(os.getenv("MEMORY_ROOT"), os.getenv("CURRENT_PROJECT_ID"), "knowledge_base.json")
    existing_chars_desc = ""
    try:
        kb = get_knowledge_base(knowledge_path)
        if kb.get("characters"):
            existing_chars_desc = "### å·²çŸ¥è§’è‰²æ³¨å†Œè¡¨ (è¯·ä¼˜å…ˆå°†æ–‡ä¸­äººç‰©æ˜ å°„åˆ°ä»¥ä¸‹æ ‡å‡†å):\n"
            for name, data in kb["characters"].items():
                aliases = data.get("aliases", [])
                desc = data.get("traits", [])[:3]
                existing_chars_desc += f"- æ ‡å‡†å: {name} | å·²çŸ¥åˆ«å: {aliases} | ç‰¹å¾: {desc}\n"
    except:
        pass

    # ä¸»è§’æ˜ å°„é€»è¾‘
    narrator_instruction = ""
    if narrator_name:
        narrator_instruction = f"""
6. **å¼ºåˆ¶æŒ‡ä»£æ¶ˆè§£**ï¼šæœ¬ç« æ˜¯ä»¥ç¬¬ä¸€äººç§°å™è¿°çš„ã€‚æ–‡ä¸­çš„ **"æˆ‘"** æŒ‡ä»£çš„æ˜¯ä¸»è§’ **"{narrator_name}"**ã€‚
   - è¾“å‡ºæ—¶è¯·ç›´æ¥ä½¿ç”¨æ ‡å‡†å "{narrator_name}"ï¼Œ**ä¸è¦** è¾“å‡º "æˆ‘"ã€‚
"""

    parser = PydanticOutputParser(pydantic_object=CharacterAnalysisResult)
    prompt_template = f"""
ä»»åŠ¡ï¼šåˆ†æä»¥ä¸‹å°è¯´ç« èŠ‚ï¼Œè¯†åˆ«æ‰€æœ‰ä¸»è¦è§’è‰²åŠä»–ä»¬çš„äº’åŠ¨å…³ç³»ã€‚

{existing_chars_desc}

### ç« èŠ‚å†…å®¹
ç¬¬{{current_chapter}}ç« ï¼š
{{text}}

### è¾“å‡ºè¦æ±‚
1. å¦‚æœæœªæåŠå±æ€§å¯ç”¨'æœªçŸ¥'ä»£æ›¿ã€‚
2. traits, hobbiesç­‰å¯æ˜¯å¤šä¸ªå€¼ã€‚
3. **æ³›ç§°å¤„ç†**ï¼šå¯¹äº"ä¸€ç¾¤ç§‘å­¦å®¶"ã€"è·¯äºº"ç­‰éå…·ä½“ç¾¤ä½“ï¼Œé™¤éæ˜¯å…³é”®è§’è‰²ï¼Œå¦åˆ™**ä¸è¦**æå–ã€‚
{narrator_instruction}

{{format_instructions}}
"""
    master_llm = get_evaluation_llm()

    try:
        prompt = PromptTemplate(
            template=prompt_template,
            input_variables=["text", "current_chapter"],
            partial_variables={"format_instructions": parser.get_format_instructions()}
        )
        chain = prompt | master_llm
        raw_response = chain.invoke({"text": text, "current_chapter": current_chapter})

        from helper import filter_think_tags, parse_json_with_filtering
        filtered_content = filter_think_tags(raw_response.content)
        parsed_result = parse_json_with_filtering(filtered_content, parser)

        result = {
            "characters": [char.model_dump() for char in parsed_result.characters],
            "character_interactions": [interaction.model_dump() for interaction in parsed_result.character_interactions]
        }
        print(f"âœ… è§’è‰²è¯†åˆ«æˆåŠŸï¼š{len(result['characters'])}ä¸ªè§’è‰²ï¼Œ{len(result['character_interactions'])}æ¬¡äº’åŠ¨")
        return result
    except Exception as e:
        print(f"--- âŒ è§’è‰²è¯†åˆ«è§£æå¤±è´¥: {e} ---")
        return {"characters": [], "character_interactions": []}


def create_character_if_not_exists(name: str, age: Optional[str] = None,
                                   gender: Optional[str] = None,
                                   appearance: str = "", traits: List[str] = None,
                                   specialties: List[str] = None, hobbies: List[str] = None,
                                   occupations: List[str] = None, current_chapter: int = 0,
                                   relationship: Dict[str, Dict[str, str]] = None,
                                   aliases: List[str] = None) -> str:
    """ç¬¬äºŒæ­¥ï¼šåˆ›å»ºè§’è‰²ï¼ˆåŒæ­¥æ›´æ–°å›¾è°±ï¼‰"""
    dotenv.load_dotenv()
    knowledge_path = os.path.join(os.getenv("MEMORY_ROOT"), os.getenv("CURRENT_PROJECT_ID"), "knowledge_base.json")
    kb = get_knowledge_base(knowledge_path)
    traits = traits or []
    relationship = relationship or {}
    aliases = aliases or []

    msg = ""
    # 1. æ›´æ–° JSON
    if name not in kb["characters"]:
        kb["characters"][name] = {
            "name": name, "age": age, "gender": gender, "backstory": "",
            "traits": traits, "appearance": appearance, "relationship": relationship,
            "first_appearance_chapter": current_chapter, "specialties": specialties,
            "hobbies": hobbies, "occupations": occupations, "aliases": aliases
        }
        msg = f"âœ… æˆåŠŸåˆ›å»ºè§’è‰² '{name}'"
    else:
        char = kb["characters"][name]
        if "aliases" not in char: char["aliases"] = []
        new_aliases = [a for a in aliases if a not in char["aliases"] and a != name]
        if new_aliases: char["aliases"].extend(new_aliases)
        msg = f"âœ… è§’è‰² '{name}' å·²å­˜åœ¨"

    save_knowledge_base(kb, knowledge_path)

    # 2. æ›´æ–° Graph
    try:
        G = load_graph()
        if not G.has_node(name): G.add_node(name)
        node_attrs = {"age": age, "gender": gender, "traits": traits, "id": name}
        for k, v in node_attrs.items():
            if v: G.nodes[name][k] = v
        save_graph(G)
    except Exception as e:
        print(f"âš ï¸ å›¾è°±èŠ‚ç‚¹æ›´æ–°å¼‚å¸¸: {e}")

    return msg


# ... (update_character_backstory å’Œ update_relationship ä¿æŒä¸å˜ï¼Œæ³¨æ„ load_graph å·²æ›´æ–°) ...
def update_character_backstory(name: str, new_information: str,
                               mode: Literal['append', 'overwrite'] = 'append',
                               current_chapter: Optional[int] = None) -> str:
    """æ›´æ–°è§’è‰²ä¸ªäººèƒŒæ™¯æ•…äº‹"""
    dotenv.load_dotenv()
    knowledge_path = os.path.join(os.getenv("MEMORY_ROOT"), os.getenv("CURRENT_PROJECT_ID"), "knowledge_base.json")
    kb = get_knowledge_base(knowledge_path)

    if name not in kb["characters"]: return f"âŒ è§’è‰² '{name}' ä¸å­˜åœ¨"

    formatted_info = f"ç¬¬{current_chapter}ç« ï¼š{new_information}" if current_chapter else new_information
    char = kb["characters"][name]

    if mode == 'append':
        char["backstory"] = (char["backstory"] + f"\n{formatted_info}") if char["backstory"] else formatted_info
    elif mode == 'overwrite':
        char["backstory"] = formatted_info

    save_knowledge_base(kb, knowledge_path)
    return f"âœ… æ›´æ–°èƒŒæ™¯æ•…äº‹: {name}"


def update_relationship(character_a: str, character_b: str,
                        new_status: str,
                        event_summary: str, chapter_evidence: str,
                        current_chapter: int) -> str:
    """ç¬¬å››æ­¥ï¼šæ›´æ–°è§’è‰²å…³ç³»ï¼ˆåŒæ­¥æ›´æ–°å›¾è°±ï¼‰"""
    dotenv.load_dotenv()
    knowledge_path = os.path.join(os.getenv("MEMORY_ROOT"), os.getenv("CURRENT_PROJECT_ID"), "knowledge_base.json")
    kb = get_knowledge_base(knowledge_path)

    if character_a not in kb["characters"] or character_b not in kb["characters"]:
        return f"âŒ é”™è¯¯ï¼šè§’è‰²ä¸å­˜åœ¨ (è¯·ç¡®ä¿ {character_a} å’Œ {character_b} éƒ½å·²åˆ›å»º)"

    # 1. æ›´æ–°å¸¸è§„çŸ¥è¯†åº“
    current_relationship_dict = kb["characters"][character_a]["relationship"].get(character_b, {})
    current_status = next(iter(current_relationship_dict.keys()), None)

    if new_status != current_status:
        relationship_entry = {new_status: f"{event_summary} (ç¬¬{current_chapter}ç« )"}
        kb["characters"][character_a]["relationship"][character_b] = relationship_entry
        kb["characters"][character_b]["relationship"][character_a] = relationship_entry
        save_knowledge_base(kb, knowledge_path)

        # 2. åŒæ­¥æ›´æ–° NetworkX çŸ¥è¯†å›¾è°±
        try:
            G = load_graph()
            if not G.has_node(character_a): G.add_node(character_a)
            if not G.has_node(character_b): G.add_node(character_b)
            G.add_edge(character_a, character_b,
                       relation=new_status,
                       summary=event_summary,
                       chapter=current_chapter)
            save_graph(G)
        except Exception as e:
            print(f"âš ï¸ å›¾è°±å…³ç³»æ›´æ–°å¼‚å¸¸: {e}")

        return f"âœ… å…³ç³»æ›´æ–°ï¼š{character_a} â†” {character_b}ï¼ˆ{new_status}ï¼‰"
    else:
        return f"â„¹ï¸  å…³ç³»æœªå˜ï¼Œè·³è¿‡æ›´æ–°"


# ... (detect_personal_backstory_updates, analyze_relationship_changes ä¿æŒä¸å˜) ...
def detect_personal_backstory_updates(characters: List[Dict], chapter_text: str, current_chapter: int) -> List[Dict]:
    """æ£€æµ‹å¹¶ç”Ÿæˆä¸ªäººèƒŒæ™¯æ•…äº‹æ›´æ–°ä»»åŠ¡"""
    tool_calls = []
    parser = PydanticOutputParser(pydantic_object=BackstorySummary)
    prompt_template = """
ä»»åŠ¡ï¼šåˆ†æä»¥ä¸‹ç« èŠ‚å†…å®¹ï¼Œæå–è§’è‰²'{name}'çš„ä¸ªäººèƒŒæ™¯æ•…äº‹ä¿¡æ¯ã€‚
ç« èŠ‚å†…å®¹ï¼š{chapter_text}
è¦æ±‚ï¼šç®€çŸ­æ€»ç»“ï¼ˆ<100å­—ï¼‰ï¼Œæ— ä¿¡æ¯è¿”"æ— "ã€‚
{format_instructions}
"""
    master_llm = get_evaluation_llm()

    for char in characters:
        name = char.get("name", "").strip()
        if not name: continue
        try:
            prompt = PromptTemplate(
                template=prompt_template,
                input_variables=["name", "chapter_text"],
                partial_variables={"format_instructions": parser.get_format_instructions()}
            )
            chain = prompt | master_llm
            raw_response = chain.invoke({"name": name, "chapter_text": chapter_text[:1500]})
            from helper import filter_think_tags, parse_json_with_filtering
            filtered_content = filter_think_tags(raw_response.content)
            parsed_summary = parse_json_with_filtering(filtered_content, parser)
            summary = parsed_summary.summary.strip()
        except Exception:
            summary = "æ— "

        if summary and summary != "æ— ":
            tool_calls.append({
                "name": "update_character_backstory",
                "args": {"name": name, "new_information": summary, "mode": "append", "current_chapter": current_chapter}
            })
    return tool_calls


def analyze_relationship_changes(character_interactions: List[Dict], chapter_text: str, current_chapter: int) -> List[
    Dict]:
    """ç¬¬ä¸‰æ­¥ï¼šåˆ†æå…³ç³»å˜åŒ–"""
    tool_calls = []
    if not character_interactions: return tool_calls

    parser = PydanticOutputParser(pydantic_object=EvidenceText)
    prompt_template = """
ä»»åŠ¡ï¼šæå–è§’è‰²'{char_a}'å’Œ'{char_b}'äº’åŠ¨çš„å…·ä½“æ–‡æœ¬ç‰‡æ®µã€‚
ç« èŠ‚å†…å®¹ï¼š{chapter_text}
{format_instructions}
"""
    master_llm = get_evaluation_llm()

    for interaction in character_interactions:
        char_a = interaction.get("character_a", "").strip()
        char_b = interaction.get("character_b", "").strip()
        interaction_type = interaction.get("interaction_type", "é™Œç”Ÿäºº").strip()
        summary = interaction.get("interaction_summary", "æ— æè¿°").strip()

        if not char_a or not char_b: continue

        try:
            prompt = PromptTemplate(
                template=prompt_template,
                input_variables=["char_a", "char_b", "chapter_text"],
                partial_variables={"format_instructions": parser.get_format_instructions()}
            )
            chain = prompt | master_llm
            raw_response = chain.invoke({
                "char_a": char_a, "char_b": char_b, "chapter_text": chapter_text[:1000]
            })
            from helper import filter_think_tags, parse_json_with_filtering
            filtered_content = filter_think_tags(raw_response.content)
            parsed_evidence = parse_json_with_filtering(filtered_content, parser)
            evidence = parsed_evidence.evidence.strip() or "æœªæå–"
        except Exception:
            evidence = "æœªæå–"

        tool_calls.append({
            "name": "update_relationship",
            "args": {
                "character_a": char_a, "character_b": char_b,
                "new_status": interaction_type, "event_summary": summary,
                "chapter_evidence": evidence, "current_chapter": current_chapter
            }
        })
    return tool_calls


def run_complete_relationship_analysis(chapter_text: str, current_chapter: int = 1):
    """å®Œæ•´çš„è§’è‰²å’Œå…³ç³»åˆ†ææµç¨‹ (ä¿®å¤ç‰ˆ)"""
    print(f"\n{'=' * 50}\nğŸ” å¼€å§‹åˆ†æç¬¬{current_chapter}ç« \n{'=' * 50}")

    # 1. è¯†åˆ«è§’è‰²
    identification_result = identify_characters_in_text(chapter_text, current_chapter)
    characters = identification_result.get("characters", [])
    interactions = identification_result.get("character_interactions", [])

    character_tool_calls = []
    # è®°å½•å·²ç»å‡†å¤‡åˆ›å»ºçš„è§’è‰²åï¼Œé˜²æ­¢é‡å¤
    chars_to_create = set()

    # 2. å¤„ç†LLMæ˜ç¡®è¯†åˆ«å‡ºçš„è§’è‰²
    for char in characters:
        name = char.get("name", "").strip()
        if name:
            chars_to_create.add(name)
            character_tool_calls.append({
                "name": "create_character_if_not_exists",
                "args": {
                    "name": name,
                    "age": char.get("estimated_age"),
                    "gender": char.get("gender"),
                    "appearance": (char.get("appearance") or "").strip(),
                    "traits": char.get("traits", []),
                    "specialties": char.get("specialties", []),
                    "hobbies": char.get("hobbies", []),
                    "occupations": char.get("occupations", []),
                    "aliases": char.get("aliases", []),
                    "current_chapter": current_chapter,
                    "relationship": {}
                }
            })

    # 3. ã€å…³é”®ä¿®å¤ã€‘æ£€æŸ¥äº’åŠ¨ä¸­æ˜¯å¦æœ‰æœªåˆ›å»ºçš„è§’è‰²ï¼Œè‡ªåŠ¨è¡¥å…¨
    for interaction in interactions:
        for key in ["character_a", "character_b"]:
            char_name = interaction.get(key, "").strip()
            if char_name and char_name not in chars_to_create:
                print(f"âš ï¸ è‡ªåŠ¨è¡¥å…¨æ¼ç½‘è§’è‰²åˆ›å»ºä»»åŠ¡: {char_name}")
                chars_to_create.add(char_name)
                character_tool_calls.append({
                    "name": "create_character_if_not_exists",
                    "args": {
                        "name": char_name,
                        "age": "æœªçŸ¥", "gender": "æœªçŸ¥", "appearance": "æœªçŸ¥",
                        "traits": [], "specialties": [], "hobbies": [], "occupations": [], "aliases": [],
                        "current_chapter": current_chapter, "relationship": {}
                    }
                })

    print(f"ğŸ“‹ ç”Ÿæˆè§’è‰²æ“ä½œä»»åŠ¡ï¼š{len(character_tool_calls)}ä¸ª")

    # 4. èƒŒæ™¯æ•…äº‹å’Œå…³ç³»æ›´æ–°
    backstory_tool_calls = detect_personal_backstory_updates(characters, chapter_text, current_chapter)
    relationship_tool_calls = analyze_relationship_changes(interactions, chapter_text, current_chapter)

    # å¿…é¡»ä¿è¯ character_tool_calls åœ¨æœ€å‰é¢æ‰§è¡Œ
    all_tool_calls = character_tool_calls + backstory_tool_calls + relationship_tool_calls
    print(f"\nâœ… åˆ†æå®Œæˆï¼šå…±ç”Ÿæˆ {len(all_tool_calls)} ä¸ªæ“ä½œä»»åŠ¡")
    return all_tool_calls


# ... (execute_tool_calls, simulate_user_confirmation ç­‰ä¿æŒåŸæ ·) ...
def execute_tool_calls(tool_calls: List[Dict]):
    """æ‰§è¡Œå·¥å…·è°ƒç”¨"""
    available_tools = {
        "create_character_if_not_exists": create_character_if_not_exists,
        "update_character_backstory": update_character_backstory,
        "update_relationship": update_relationship,
        "merge_characters": merge_characters  # æ³¨å†Œåˆå¹¶å·¥å…·
    }
    results = []
    if not tool_calls: return results

    print(f"\nğŸ› ï¸  å¼€å§‹æ‰§è¡Œ {len(tool_calls)} ä¸ªå·¥å…·è°ƒç”¨...")
    for idx, call in enumerate(tool_calls, 1):
        print(f"\n--- ä»»åŠ¡{idx}/{len(tool_calls)}ï¼š{call['name']} ---")
        if call["name"] not in available_tools:
            print(f"âŒ æœªçŸ¥å·¥å…·ï¼š{call['name']}")
            continue
        try:
            tool_func = available_tools[call["name"]]
            result = tool_func(**call["args"])
            print(f"âœ… ç»“æœï¼š{result}")
            results.append(result)
        except Exception as e:
            print(f"âŒ æ‰§è¡Œå¤±è´¥ï¼š{str(e)}")
    return results


def simulate_user_confirmation_and_execute(tool_calls: List[Dict]):
    # ... (ä¸åŸä»£ç ä¸€è‡´) ...
    if not tool_calls:
        print("â„¹ï¸  æ— å¾…æ‰§è¡Œæ“ä½œ")
        return []
    return execute_tool_calls(tool_calls)


# ================================================================= #
# 5. å›¾è°±æ¨ç†ä¸åˆå¹¶å·¥å…· (æ–°åŠ )
# ================================================================= #

def merge_characters(primary_name: str, alias_name: str) -> str:
    """åˆå¹¶ä¸¤ä¸ªå®ä½“èŠ‚ç‚¹ (ä¾‹å¦‚ 'æˆ‘' -> 'æ—å¤')"""
    print(f"--- ğŸ”„ å¼€å§‹åˆå¹¶å®ä½“: '{alias_name}' -> '{primary_name}' ---")
    dotenv.load_dotenv()
    kb_path = os.path.join(os.getenv("MEMORY_ROOT"), os.getenv("CURRENT_PROJECT_ID"), "knowledge_base.json")

    try:
        kb = get_knowledge_base(kb_path)
        if primary_name not in kb["characters"]: return f"âŒ ä¸»è§’è‰² '{primary_name}' ä¸å­˜åœ¨"
        if alias_name not in kb["characters"]: return f"â„¹ï¸  åˆ«åè§’è‰² '{alias_name}' ä¸å­˜åœ¨ï¼Œè·³è¿‡"

        p_data = kb["characters"][primary_name]
        a_data = kb["characters"][alias_name]

        # åˆå¹¶å±æ€§
        for field in ["traits", "specialties", "hobbies", "occupations"]:
            if a_data.get(field):
                p_data[field] = list(set(p_data.get(field, []) + a_data[field]))

        # åˆå¹¶èƒŒæ™¯
        if a_data.get("backstory"):
            p_data["backstory"] += f"\nã€æ¥è‡ª{alias_name}ã€‘ï¼š{a_data['backstory']}"

        # åˆå¹¶å…³ç³»
        if "relationship" in a_data:
            for target, rel in a_data["relationship"].items():
                if target == primary_name: continue
                if target not in p_data["relationship"]:
                    p_data["relationship"][target] = rel
                    # åå‘æ›´æ–°å¯¹æ–¹çš„å…³ç³»æŒ‡å‘
                    if target in kb["characters"] and alias_name in kb["characters"][target]["relationship"]:
                        old_rel = kb["characters"][target]["relationship"].pop(alias_name)
                        kb["characters"][target]["relationship"][primary_name] = old_rel

        # è®°å½•åˆ«å
        if "aliases" not in p_data: p_data["aliases"] = []
        if alias_name not in p_data["aliases"]: p_data["aliases"].append(alias_name)

        del kb["characters"][alias_name]
        save_knowledge_base(kb, kb_path)

        # å›¾è°±åˆå¹¶
        G = load_graph()
        if G.has_node(alias_name):
            if not G.has_node(primary_name): G.add_node(primary_name, **G.nodes[alias_name])
            for n in list(G.neighbors(alias_name)):
                if n == primary_name: continue
                if not G.has_edge(primary_name, n):
                    G.add_edge(primary_name, n, **G.get_edge_data(alias_name, n))
            G.remove_node(alias_name)
            save_graph(G)

        return f"âœ… æˆåŠŸåˆå¹¶: {alias_name} -> {primary_name}"
    except Exception as e:
        return f"âŒ åˆå¹¶å‡ºé”™: {e}"


def get_story_graph_context(focus_characters: List[str]) -> str:
    """å›¾è°±æ¨ç†å¼•æ“"""
    try:
        G = load_graph()
        if G.number_of_nodes() == 0: return "æš‚æ— å›¾è°±æ•°æ®ã€‚"
        valid_chars = [c for c in focus_characters if G.has_node(c)]
        if not valid_chars: return "æ— ç›¸å…³å›¾è°±ä¿¡æ¯ã€‚"

        context = []
        if len(valid_chars) == 1:
            char = valid_chars[0]
            context.append(f"### ã€{char}ã€‘çš„ç¤¾äº¤åœˆ")
            for n in G.neighbors(char):
                ed = G.get_edge_data(char, n)
                context.append(f"- {n} ({ed.get('relation', '')}): {ed.get('summary', '')}")
        else:
            context.append("### äººç‰©å…³ç³»æ¨ç†")
            subgraph = G.subgraph(valid_chars)
            for u, v, d in subgraph.edges(data=True):
                context.append(f"- {u}<->{v}: {d.get('relation')} ({d.get('summary')})")
        return "\n".join(context)
    except:
        return "å›¾è°±æ¨ç†æœåŠ¡æš‚ä¸å¯ç”¨ã€‚"


if __name__ == "__main__":
    pass